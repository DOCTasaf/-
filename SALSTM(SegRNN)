'''
SALSTM: segmented self‑attention long short‑term memory for long‑term forecasting
'''
import torch.nn as nn
import torch.nn.functional as F
import torch
import time
import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import scipy.io as sio
from torch.nn.utils import weight_norm
import torch.nn.functional as F
import itertools
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import random
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from torch.nn import LayerNorm


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")




def seed_torch(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)


seed_torch(2048)
torch.cuda.empty_cache()



class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):
        """
        :param num_features: the number of features or channels
        :param eps: a testue added for numerical stability
        :param affine: if True, RevIN has learnable affine parameters
        """
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        self.subtract_last = subtract_last
        if self.affine:
            self._init_params()

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        else: raise NotImplementedError
        return x

    def _init_params(self):
        # initialize RevIN params: (C,)
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def _get_statistics(self, x):
        dim2reduce = tuple(range(1, x.ndim-1))
        if self.subtract_last:
            self.last = x[:,-1,:].unsqueeze(1)
        else:
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        if self.subtract_last:
            x = x - self.last
        else:
            x = x - self.mean
        x = x / self.stdev
        if self.affine:
            x = x * self.affine_weight
            x = x + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = x - self.affine_bias
            x = x / (self.affine_weight + self.eps*self.eps)
        x = x * self.stdev
        if self.subtract_last:
            x = x + self.last
        else:
            x = x + self.mean
        return x


class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadSelfAttention, self).__init__()
        assert embed_dim % num_heads == 0, "Embedding dimension must be divisible by number of heads"

        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads  # d' = d / h

        # 线性变换 Q, K, V
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)

        # 最终的线性层
        self.W_o = nn.Linear(embed_dim, embed_dim)

        # ReLU 激活函数
        self.relu = nn.ReLU()

    def scaled_dot_product_attention(self, Q, K, V):
        """
        计算 Scaled Dot-Product Attention
        """
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
        attn_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, V)
        return output

    def forward(self, x):
        batch_size, seq_len, embed_dim = x.shape

        # 线性变换后 reshape 成 (batch, seq_len, num_heads, head_dim)
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # 交换维度 (batch, num_heads, seq_len, head_dim) 以便计算注意力
        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)

        # 计算注意力
        attention_output = self.scaled_dot_product_attention(Q, K, V)

        # 使用 ReLU 进行非线性变换
        attention_output = self.relu(attention_output)

        # 交换回原来的维度 (batch, seq_len, num_heads, head_dim)
        attention_output = attention_output.transpose(1, 2).contiguous()

        # 重新拼接维度 (batch, seq_len, embed_dim)
        concat_output = attention_output.view(batch_size, seq_len, embed_dim)

        # 通过最终的线性层
        output = self.W_o(concat_output)
        return output


class SALSTM(nn.Module):
    def __init__(self, input_size, hidden_size, H, dropout, batch_size, feature, seq_len, seg_len=10, L=200,
                 num_layer=128, enc_in=1):
        super(SALSTM, self).__init__()
        self.reshaping_success = False  # 添加标志位

        self.H = H
        self.L = L
        self.enc_in = enc_in
        self.num_layer = num_layer
        self.dropout = dropout
        self.hidden_size = hidden_size
        self.input_size = input_size
        self.seg_len = seg_len
        self.feature = feature
        self.batch_size = batch_size

        if self.H % self.seg_len != 0:
            raise ValueError(f"Input length H={self.H} must be divisible by segment length seg_len={self.seg_len}")

        self.seg_num_x = self.H // self.seg_len
        self.valueEmbedding = nn.Sequential(
            nn.Linear(self.seg_len, self.num_layer),
            nn.ReLU()
        )

        self.layer_norm = nn.LayerNorm(self.H)
        self.layernorm_rnn = nn.LayerNorm(self.num_layer)
        self.linear1 = nn.Linear(self.feature, self.seg_num_x)
        self.mhn = nn.Sequential(
            nn.Linear(self.H, self.hidden_size),
            nn.ReLU()
        )

        self.linear2 = nn.Linear(self.H, self.hidden_size)
        self.MultiHeadSelfAttention = MultiHeadSelfAttention(self.hidden_size, 4)
        self.rnn = nn.LSTM(input_size=self.num_layer, hidden_size=self.num_layer, num_layers=1, bias=True,
                           batch_first=True, bidirectional=False)

        self.channel_emb1 = nn.Parameter(torch.randn(self.seg_num_x, self.num_layer // 2))
        self.pos_emb1 = nn.Parameter(torch.randn(self.seg_num_x, self.num_layer // 2))

        self.seg_num_y = self.L // self.seg_len

        self.pos_emb = nn.Parameter(torch.randn(self.seg_num_y, self.num_layer // 2))
        self.channel_emb = nn.Parameter(torch.randn(self.enc_in, self.num_layer // 2))

        self.predict = nn.Sequential(
            nn.Dropout(self.dropout),
            nn.Linear(self.num_layer, self.seg_len)
        )

        self.revinLayer = RevIN(self.enc_in, affine=False, subtract_last=False)

    def forward(self, x):

        batch_size = x.size(0)
        x = x.permute(0, 2, 1)  ##### (batchsize, feature, seqlength)

        try:

            x_1 = x.view(-1, 1)
            x_1 = self.linear1(x_1)
            x_reshaped = x_1.view(-1, self.H)
            output1 = self.mhn(x_reshaped)
            x = output1.view(self.batch_size, self.seg_num_x, self.hidden_size)

        except Exception as e:
            x = self.valueEmbedding(x.reshape(-1, self.seg_num_x, self.seg_len))

        else:
            if not self.reshaping_success:
                print("Reshaping was successful, continuing with program...")
                self.reshaping_success = True
            pass

        x = self.revinLayer(x.permute(0, 2, 1), 'norm').permute(0, 2, 1)


        x11 = self.pos_emb1.unsqueeze(0).repeat(self.enc_in, 1, 1)

        print(f"x11: {x11.shape}")  ####### (batchsize, self.seg_num_x, hidden_size)

        pos_emb11 = self.pos_emb1.unsqueeze(0).repeat(batch_size, 1, 1)  # 位置编码
        print(f"pos_emb11: {pos_emb11.shape}")  ####### (batchsize, self.seg_num_x, hidden_size)

        channel_emb11 = self.channel_emb1.unsqueeze(0).repeat(batch_size, 1, 1)  # 通道编码
        print(f"channel_emb11: {channel_emb11.shape}")  ####### (batchsize, self.seg_num_x, hidden_size//2)

        encoding = torch.cat([pos_emb11, channel_emb11], dim=-1)  # 合并编码
        print(f"encoding: {encoding.shape}")  ####### (batchsize, self.seg_num_x, hidden_size//2)

        x = self.MultiHeadSelfAttention(x)

        print(f"x: {x.shape}")  ####### (batchsize, self.seg_num_x, hidden_size)

        x = x + encoding


        ########################     lstm encoder ################################
        _, (hn, cn) = self.rnn(x)
        hn = self.layernorm_rnn(hn)
        hn = torch.relu(hn)
        hn = F.dropout(hn, p=self.dropout, training=self.training)  ##([self.enc_in, batchsize, hidden_size])

        pos_emb = torch.cat([
            self.pos_emb.unsqueeze(0).repeat(self.enc_in, 1, 1),
            self.channel_emb.unsqueeze(1).repeat(1, self.seg_num_y, 1)
        ], dim=-1).view(-1, 1, self.num_layer).repeat(batch_size, 1, 1)


        #################################   LSTM decoder #################
        _, (hy, cy) = self.rnn(pos_emb,
                               (hn.repeat(1, 1, self.seg_num_y).view(1, -1, self.num_layer),
                                cn.repeat(1, 1, self.seg_num_y).view(1, -1, self.num_layer)))


        y = self.predict(hy).view(-1, self.enc_in, self.L)
        print("y.shape", y.shape)

        y = self.revinLayer(y.permute(0, 2, 1), 'denorm').permute(0, 2, 1)

        y = y[:, :self.enc_in, :] #####强制转换

        y = y.permute(0, 2, 1)
        return y


input_size = 1
hidden_size = 32
kernel_size = 3
H = 400
L = 200
batch_size = 100
seq_len = H
seg_len = 20
dropout = 0.2

model = SALSTM(input_size=input_size, H=H, L=L, hidden_size=hidden_size, dropout=dropout,
               seq_len=seq_len, batch_size=batch_size, num_layer=hidden_size,
               feature=1, seg_len=seg_len)

x = torch.randn(batch_size, H, 1)
model = model.to(device)
total_params = sum(p.numel() for p in model.parameters())
total_params += sum(p.numel() for p in model.buffers())
print(f'{total_params:,} total parameters.')

output = model(x)
print(output.shape)  # 输出的形状为 (batch_size, L, input_size)
